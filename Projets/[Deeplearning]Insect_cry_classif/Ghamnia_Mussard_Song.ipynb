{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9V9Q7_D9cnih"
   },
   "source": [
    "# Classification de cris d'insectes\n",
    "\n",
    "Les populations d’insectes terrestres subissent actuellement un déclin massif d’environ 10% par décennie. Il est urgent de mieux comprendre les causes de ce déclin et surtout d’identifier les méthodes de gestion de l'environnement permettant de le limiter. Néanmoins la recherche traditionnelle en entomologie est peu outillée pour l’étude non invasive (sans capture et sans mise à mort) et à haute fréquence spatio-temporelle des milliers d’espèces qui caractérisent cette classe très diversifiée du règne animal.\n",
    "\n",
    "Pour ces raisons, des approches de suivi des populations basées IA et capteurs émergent ; l'idée est d'utiliser des enregistreurs audio de type *soundscape* pour écouter et reconnaître la présence et la fréquence de cris des différentes espèces.\n",
    "\n",
    "Dans ce projet, vous devrez mettre en place un réseau de neurones pour la reconnaissance d'espèces d'insectes chanteurs (cigales, grillons, sauterelles, criquets) à partir de courts échantillons audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import audiomentations as AA\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "from python_speech_features import mfcc\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des audios et transformation en dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InsectImage(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, data_type, transform, list_augment):\n",
    "        #On récupère le fichier csv\n",
    "        self.img_labels = pd.read_csv(annotations_file, sep=',')\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "        # On ne fait qu'un nombre fixé d'augmentations selon le nb d'occurences des classes\n",
    "        self.data_type = self.img_labels[\n",
    "            (self.img_labels['class_ID'].isin(list_augment)) &\n",
    "            (self.img_labels['data_set'] == data_type)\n",
    "        ]\n",
    "        # transformation pour l'augmentation de donnée \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_type)\n",
    "\n",
    "    def decoupage_segments(self, audio_path, duree_segment=5):\n",
    "        \"\"\"\n",
    "        Méthode pour extraire des segments de 5 secondes pour chaque audio et remplissage avec des 0 pour le dernier segment\n",
    "        si il ne dure pas 5 secondes.\n",
    "        En sortie on a une liste comportant tous les segments de 5 secondes de l'audio actuel\n",
    "        \"\"\"\n",
    "        audio, sr = librosa.load(audio_path, sr=None)\n",
    "        frames_par_segment = int(sr * duree_segment)\n",
    "        nombre_segments = int(np.ceil(len(audio) / frames_par_segment))\n",
    "        segments = []\n",
    "        for i in range(nombre_segments):\n",
    "            segment_audio = audio[i * frames_par_segment: (i + 1) * frames_par_segment]\n",
    "            dernier_segment_taille = len(segment_audio)\n",
    "            if dernier_segment_taille < frames_par_segment:\n",
    "                segment_audio = np.pad(segment_audio, (0, frames_par_segment - dernier_segment_taille), mode='constant')\n",
    "            segments.append(segment_audio)\n",
    "        return segments, sr\n",
    "\n",
    "    def augmentation(self, segment):\n",
    "        \"\"\"\n",
    "        Méthode permettant d'appliquer la transformation choisit par l'utilisateur\n",
    "        \"\"\"\n",
    "        if self.transform=='left_shift':\n",
    "            S_db = AA.Shift(min_shift=-0.5, max_shift=-0.1, p=1)(segment, sample_rate=44100)\n",
    "        elif self.transform=='right_shift':\n",
    "            S_db = AA.Shift(min_shift=0.1, max_shift=0.5, p=1)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'slow_stretch':\n",
    "            S_db = AA.TimeStretch(min_rate=0.7, max_rate=0.9, leave_length_unchanged=True)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'accelerate_stretch':\n",
    "            S_db = AA.TimeStretch(min_rate=1.7, max_rate=1.9, leave_length_unchanged=True)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'few_noise':\n",
    "            S_db = AA.AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'lot_noise':\n",
    "            S_db = AA.AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.15, p=1)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'lower_pitch':\n",
    "            S_db =AA.PitchShift(min_semitones=-2, max_semitones=-1)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'higher_pitch':\n",
    "            S_db =AA.PitchShift(min_semitones=1, max_semitones=2)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'time_mask':\n",
    "            S_db = AA.TimeMask(min_band_part=0.1,max_band_part=0.15,fade=True,p=1.0)(segment, sample_rate=44100)\n",
    "        elif self.transform == 'reverse':\n",
    "            S_db = AA.Reverse(p=1.0)(segment, sample_rate=44100)\n",
    "        else :\n",
    "            S_db = segment\n",
    "        return S_db\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Pour chaque audio et chaque segment de l'audio on applique la transfromation et on transforme le signal en \n",
    "        un spectrogramme\n",
    "        \"\"\"\n",
    "        audio_path = os.path.join(self.img_dir, self.data_type.iloc[idx, 1], self.data_type.iloc[idx, 0])\n",
    "        ## Récupération du label associé à l'audio\n",
    "        label = self.data_type.iloc[idx, 2]\n",
    "        #Découpage de l'audio en segments\n",
    "        segments, sr = self.decoupage_segments(audio_path)\n",
    "        ##On compte le nb de segments pour l'audio (pour pouvoir ensuite calculer le nb d'occurences par classe)\n",
    "        longueur_segment = len(segments)\n",
    "\n",
    "        for segment in segments:\n",
    "            ## On applique l'augmentation\n",
    "            S_db = self.augmentation(segment)\n",
    "            #On applique la stft pour avoir un spectrogramme\n",
    "            D = librosa.stft(S_db.astype('float32'))\n",
    "            S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "            ##Une autre transformation possible sur le spectrogramme\n",
    "            if self.transform == 'freq_mask':\n",
    "                S_db = AA.SpecFrequencyMask(p=1.0)(S_db)\n",
    "            S_db = torch.from_numpy(S_db)\n",
    "            S_db = torch.unsqueeze(S_db, dim=0)\n",
    "            \n",
    "            return S_db, label, longueur_segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le chemin 'projetmodia' correspond au dossier qui contient les données, nous vous conseillons de le mettre dans le même dossier que ce notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# récupérer le chemin du répertoire courant\n",
    "path = os.getcwd()\n",
    "path = path + \"/projetmodia/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du nombre d'occurences sans ré-équilibrage des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Liste avec tous les labels\n",
    "list_all_Labels = list(range(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_train_cicadae = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', data_type='train', transform = None, list_augment= list_all_Labels)\n",
    "dataset_train_orthoptera = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', data_type='train', transform = None, list_augment = list_all_Labels)\n",
    "train = torch.utils.data.ConcatDataset([dataset_train_cicadae, dataset_train_orthoptera])\n",
    "train_dataloader2 = DataLoader(train, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_occ = {}\n",
    "for _, data in enumerate(train_dataloader2):\n",
    "    nb_donnees = data[2].item()\n",
    "    class_label=data[1].item()\n",
    "    if class_label in class_occ.keys():\n",
    "        class_occ[class_label]+= nb_donnees\n",
    "    else:\n",
    "        class_occ[class_label]=nb_donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(class_occ.items()))\n",
    "print(sorted_dict)\n",
    "dataframe = pd.DataFrame.from_dict(sorted_dict,  orient='index', columns = ['Occurences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 'Azanicadazuluensis',\n",
    "    1: 'Brevisianabrevis',\n",
    "    2: 'Chorthippusbiguttulus',\n",
    "    3: 'Chorthippusbrunneus',\n",
    "    4: 'Grylluscampestris',\n",
    "    5: 'Kikihiamuta',\n",
    "    6: 'Myopsaltaleona',\n",
    "    7: 'Myopsaltalongicauda',\n",
    "    8: 'Myopsaltamackinlayi',\n",
    "    9: 'Myopsaltamelanobasis',\n",
    "    10: 'Myopsaltaxerograsidia',\n",
    "    11: 'Nemobiussylvestris',\n",
    "    12: 'Oecanthuspellucens',\n",
    "    13: 'Pholidopteragriseoaptera',\n",
    "    14: 'Platypleuracapensis',\n",
    "    15: 'Platypleuracfcatenata',\n",
    "    16: 'Platypleurachalybaea',\n",
    "    17: 'Platypleuradeusta',\n",
    "    18: 'Platypleuradivisa',\n",
    "    19: 'Platypleurahaglundi',\n",
    "    20: 'Platypleurahirtipennis',\n",
    "    21: 'Platypleuraintercapedinis',\n",
    "    22: 'Platypleuraplumosa',\n",
    "    23: 'Platypleurasp04',\n",
    "    24: 'Platypleurasp10',\n",
    "    25: 'Platypleurasp11cfhirtipennis',\n",
    "    26: 'Platypleurasp12cfhirtipennis',\n",
    "    27: 'Platypleurasp13',\n",
    "    28: 'Pseudochorthippusparallelus',\n",
    "    29: 'Pycnasemiclara',\n",
    "    30: 'Roeselianaroeselii',\n",
    "    31: 'Tettigoniaviridissima'\n",
    "}\n",
    "\n",
    "# Appliquez la correspondance à la colonne des numéros de classes\n",
    "dataframe['class_names'] = class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.bar(dataframe, x='class_names', y='Occurences', title='Répartition des classes')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que certaines classes ont beaucoup plus d'occurences que d'autres et cela peut impacter la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous appliquons ensuite de l'augmentation de données pour à la fois avoir plus d'exemples car le dataset est petit mais aussi pour ré-équilibrer les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Toutes les transformations possibles\n",
    "transformations = ['left_shift', 'right_shift', 'slow_stretch', 'accelerate_stretch', 'few_noise', 'lot_noise','lower_pitch', 'higher_pitch', 'higher_pitch', 'time_mask', 'freq_mask', 'reverse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calcul quartiles pour augmentation de données\n",
    "q1 = dataframe['Occurences'].quantile(0.25)\n",
    "median = dataframe['Occurences'].median()\n",
    "q3 = dataframe['Occurences'].quantile(0.75)\n",
    "\n",
    "# Listes des classes pour l'augmentations de données\n",
    "classes_q1 = dataframe[dataframe['Occurences'] <= q1].index.tolist()\n",
    "classes_q1_median = dataframe[(dataframe['Occurences'] > q1) & (dataframe['Occurences'] <= median)].index.tolist()\n",
    "classes_median_q3 = dataframe[(dataframe['Occurences'] > median) & (dataframe['Occurences'] <= q3)].index.tolist()\n",
    "classes_q3 = dataframe[dataframe['Occurences'] > q3].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# On effectue les 10 transformations pour les classes qui ont un nb d'occurences inférieurs au 1er quartile\n",
    "train_cicadae1 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q1, data_type='train', transform = None)\n",
    "train_orthoptera1 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q1, data_type='train', transform = None)\n",
    "train_dataset1 = torch.utils.data.ConcatDataset([train_cicadae1, train_orthoptera1])\n",
    "transformQ1_list = random.sample(transformations, 11)\n",
    "for transform in transformQ1_list :\n",
    "    dataset_train_cicadae1 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q1, data_type='train', transform = transform)\n",
    "    dataset_train_orthoptera1 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q1, data_type='train', transform = transform)\n",
    "    train_dataset1 = torch.utils.data.ConcatDataset([train_dataset1, dataset_train_cicadae1, dataset_train_orthoptera1])\n",
    "  \n",
    "\n",
    "# On effectue 6 transformations choisis aléatoirement sans remise pour les classes qui ont un nb d'occurences compris entre le 1er quartile et la médiane\n",
    "train_cicadae2 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q1_median, data_type='train', transform = None)\n",
    "train_orthoptera2 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q1_median, data_type='train', transform = None)\n",
    "train_dataset2 = torch.utils.data.ConcatDataset([train_cicadae2, train_orthoptera2])\n",
    "transformQ2_list = random.sample(transformations, 6)\n",
    "for transform in transformQ2_list :\n",
    "    dataset_train_cicadae2 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q1_median, data_type='train', transform = transform)\n",
    "    dataset_train_orthoptera2 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q1_median, data_type='train', transform = transform)\n",
    "    train_dataset2 = torch.utils.data.ConcatDataset([train_dataset2, dataset_train_cicadae2, dataset_train_orthoptera2])\n",
    " \n",
    "\n",
    "\n",
    "# On effectue 3 transformations choisis aléatoirement sans remise pour les classes qui ont un nb d'occurences compris entre la médiane et le 3ieme quartile\n",
    "train_cicadae3 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_median_q3, data_type='train', transform = None)\n",
    "train_orthoptera3 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_median_q3, data_type='train', transform = None)\n",
    "train_dataset3 = torch.utils.data.ConcatDataset([train_cicadae3, train_orthoptera3])\n",
    "transformQ3_list = random.sample(transformations, 3)\n",
    "for transform in transformQ3_list :\n",
    "    dataset_train_cicadae3 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_median_q3, data_type='train', transform = transform)\n",
    "    dataset_train_orthoptera3 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_median_q3, data_type='train', transform = transform)\n",
    "    train_dataset3 = torch.utils.data.ConcatDataset([train_dataset3, dataset_train_cicadae3, dataset_train_orthoptera3])\n",
    "\n",
    "\n",
    "# On effectue 1 transformation choisi aléatoirement sans remise pour les classes qui ont un nb d'occurences > au 3ieme quartile\n",
    "train_cicadae4 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q3, data_type='train', transform = None)\n",
    "train_orthoptera4 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q3, data_type='train', transform = None)\n",
    "train_dataset4 = torch.utils.data.ConcatDataset([train_cicadae4, train_orthoptera4])\n",
    "transformQ4_list = random.sample(transformations, 1)\n",
    "for transform in transformQ4_list :\n",
    "    dataset_train_cicadae4 = InsectImage(path+'Cicadidae.csv', path+'Cicadidae/Cicadidae/', list_augment=classes_q3, data_type='train', transform = transform)\n",
    "    dataset_train_orthoptera4 = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', list_augment=classes_q3, data_type='train', transform = transform)\n",
    "    train_dataset4 = torch.utils.data.ConcatDataset([train_dataset4, dataset_train_cicadae4, dataset_train_orthoptera4])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du nombre d'occurences après augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_all_augments = torch.utils.data.ConcatDataset([train_dataset1, train_dataset2, train_dataset3, train_dataset4])\n",
    "train_dataloader_complete = DataLoader(dataset_all_augments, batch_size=1, shuffle=True)\n",
    "class_occ = {}\n",
    "\n",
    "for _, data in enumerate(train_dataloader_complete):\n",
    "    nb_donnees = data[2].item()\n",
    "    class_label=data[1].item()\n",
    "    if class_label in class_occ.keys():\n",
    "        class_occ[class_label]+= nb_donnees\n",
    "    else:\n",
    "        class_occ[class_label]=nb_donnees\n",
    "\n",
    "sorted_dict = dict(sorted(class_occ.items()))\n",
    "dataframe_final = pd.DataFrame.from_dict(sorted_dict,  orient='index', columns = ['Occurences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_mapping = {\n",
    "    0: 'Azanicadazuluensis',\n",
    "    1: 'Brevisianabrevis',\n",
    "    2: 'Chorthippusbiguttulus',\n",
    "    3: 'Chorthippusbrunneus',\n",
    "    4: 'Grylluscampestris',\n",
    "    5: 'Kikihiamuta',\n",
    "    6: 'Myopsaltaleona',\n",
    "    7: 'Myopsaltalongicauda',\n",
    "    8: 'Myopsaltamackinlayi',\n",
    "    9: 'Myopsaltamelanobasis',\n",
    "    10: 'Myopsaltaxerograsidia',\n",
    "    11: 'Nemobiussylvestris',\n",
    "    12: 'Oecanthuspellucens',\n",
    "    13: 'Pholidopteragriseoaptera',\n",
    "    14: 'Platypleuracapensis',\n",
    "    15: 'Platypleuracfcatenata',\n",
    "    16: 'Platypleurachalybaea',\n",
    "    17: 'Platypleuradeusta',\n",
    "    18: 'Platypleuradivisa',\n",
    "    19: 'Platypleurahaglundi',\n",
    "    20: 'Platypleurahirtipennis',\n",
    "    21: 'Platypleuraintercapedinis',\n",
    "    22: 'Platypleuraplumosa',\n",
    "    23: 'Platypleurasp04',\n",
    "    24: 'Platypleurasp10',\n",
    "    25: 'Platypleurasp11cfhirtipennis',\n",
    "    26: 'Platypleurasp12cfhirtipennis',\n",
    "    27: 'Platypleurasp13',\n",
    "    28: 'Pseudochorthippusparallelus',\n",
    "    29: 'Pycnasemiclara',\n",
    "    30: 'Roeselianaroeselii',\n",
    "    31: 'Tettigoniaviridissima'\n",
    "}\n",
    "\n",
    "dataframe_final['class_names'] = class_mapping\n",
    "fig = px.bar(dataframe_final, x='class_names', y='Occurences', title='Répartition des classes')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous voyons que les classes sont à peu près équilibrées "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction du dataset d'entrainement, de validation et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajout de poids pour la loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weights = torch.tensor(1.0 / dataframe_final['Occurences'].values, dtype=torch.float32)\n",
    "class_weights = class_weights.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pour l'entraînement avec batch de 32 et non 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader_final = DataLoader(dataset_all_augments, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset de validation et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test_orthoptera = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', data_type='test',transform = None, list_augment = list_all_Labels)\n",
    "dataset_validation_orthoptera = InsectImage(path+'Orthoptera.csv', path+'Orthoptera/Orthoptera/', data_type='validation',transform =None, list_augment = list_all_Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test_cicadae = InsectImage(path+'Cicadidae.csv', path+ 'Cicadidae/Cicadidae/', data_type='test', transform = None, list_augment=list_all_Labels)\n",
    "dataset_validation_cicadae = InsectImage(path+'Cicadidae.csv',path+ 'Cicadidae/Cicadidae/', data_type='validation', transform =None, list_augment=list_all_Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = torch.utils.data.ConcatDataset([dataset_test_cicadae, dataset_test_orthoptera])\n",
    "val_dataset = torch.utils.data.ConcatDataset([dataset_validation_cicadae, dataset_validation_orthoptera])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il vous faudra avoir la version 0.9.12 du package timm pour pouvoir exécuter la partie suivante  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import timm\n",
    "model = timm.create_model('xception', pretrained=False, num_classes=1000)\n",
    "path = os.getcwd()\n",
    "model.conv1 = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
    "model.fc = nn.Linear(in_features=2048, out_features=32, bias=True)\n",
    "model.load_state_dict(torch.load(path+'/xception.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.001)\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "loss_values_validation = []\n",
    "accuracy_values_validation = []\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "    print('-------------')\n",
    "    running_loss = 0.0\n",
    "    n_samples =0.0\n",
    "    running_acc = 0.0\n",
    "    for i, data in  enumerate(tqdm(train_dataloader_final)):\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds= torch.argmax(outputs, dim=1)\n",
    "        running_loss += loss.item() * img.size(0)\n",
    "        n_samples += label.size(0)\n",
    "        running_acc += torch.sum(preds == label).item()\n",
    " \n",
    "    epoch_loss = running_loss / n_samples\n",
    "    epoch_accuracy = running_acc / n_samples * 100.0\n",
    "\n",
    "    loss_values.append(epoch_loss)\n",
    "    accuracy_values.append(epoch_accuracy)\n",
    "    print('Epoch [{}/{}], Loss Train: {:.4f}, Accuracy Train: {:.4f}%'.format(epoch + 1, num_epochs, epoch_loss, epoch_accuracy))\n",
    "    \n",
    "    ##### Validation loop\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    n_samples =0.0\n",
    "    running_acc_validation = 0.0\n",
    "    running_loss_validation = 0.0\n",
    "    model.eval()     \n",
    "    for i, data in enumerate(validation_dataloader):\n",
    "        img, label = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(img)\n",
    "        loss = criterion(outputs,label)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        running_loss_validation += loss.item() * img.size(0)\n",
    "        n_samples += label.size(0)\n",
    "        running_acc_validation += torch.sum(preds == label).item()\n",
    " \n",
    "    epoch_loss_validation = running_loss_validation / n_samples\n",
    "    epoch_accuracy_validation = running_acc_validation / n_samples * 100.0\n",
    "\n",
    "    loss_values_validation.append(epoch_loss_validation)\n",
    "    accuracy_values_validation.append(epoch_accuracy_validation)\n",
    "    print('Epoch [{}/{}], Loss Validation: {:.4f}, Accuracy Validation: {:.4f}%'.format(epoch + 1, num_epochs, epoch_loss_validation, epoch_accuracy_validation))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval() \n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _, data in enumerate(test_dataloader):\n",
    "        img, true_labels_batch = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(img)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        true_labels.extend(true_labels_batch.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "        total += true_labels_batch.size(0)\n",
    "        correct += (preds == true_labels_batch).sum().item()\n",
    "        \n",
    "acc = correct / total * 100.0\n",
    "print('accuracy est de seulement: {:.2f}%'.format(acc))\n",
    "\n",
    "## Calcul du f1-score : \n",
    "y_true = np.array(true_labels)\n",
    "y_pred = np.array(predicted_labels)\n",
    "\n",
    "sommaire = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "for class_label, metrics in sommaire.items():\n",
    "    if class_label.isdigit():\n",
    "        F1_score_class = metrics['f1-score']\n",
    "        print(f\"F1-score pour la classe {class_label} : {F1_score_class}\")\n",
    "        \n",
    "f1_score_moyen = sommaire['weighted avg']['f1-score']\n",
    "print(f\"F1-score moyen : {f1_score_moyen}\")\n",
    "\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# Afficher la matrice de confusion\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=range(32), yticklabels=range(32))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('matrice_confusion.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pandora_MC_CNN",
   "language": "python",
   "name": "pandora_mccnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
